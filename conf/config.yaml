defaults:
  - pipelines: simple
  - _self_
# Configuration for the individual modules. So far all modules share the same config.
# This might be moved to the pipeline definition.
app:
  # read input from argument (query) or stdin
  input: query
  # the list of the domains, that are already registered
  domains: data/suggestable_domains.csv
  # the query the names are generated for
  query: mydomain
  # the number of generated suggestions (TODO: implement)
  suggestions: 10
  # by default no pipelines are defined
  logging_level: INFO
#  secondary_market_names: data/secondary.csv
#  advertised_names: data/advertised.csv
  internet_domains: data/top_internet_names.csv
  primary_url: suggestable_domains.csv
#  secondary_url: secondary.csv
#  advertised_url: advertised.csv
  subnames_url: subnames.txt
  clubs_url: suggestable_clubs.csv
  clubs: data/suggestable_clubs.csv
  #for longer names only first 30 characters are taken into consideration
  name_length_limit: 30
  min_primary_fraction: 0.1
filtering:
  # List of already registered domains
#  domains: data/primary.csv
  subnames: data/subnames.txt
  root_path: ./
tokenization:
  dictionary: data/words.txt
  skip_one_letter_words: true
  skip_non_words: false
  add_letters_ias: true
  with_gaps: true
generation:
  word2vec_url: embeddings.pkl
  word2vec_path: data/embeddings.pkl
  suffixes_path: data/suffixes.txt
  prefixes_path: data/prefixes.txt
  limit: 10000
  wikipedia2vec_url: enwiki_20180420_100d.processed.pkl
  wikipedia2vec_path: data/wikipedia2vec.pkl
sorting:
  weighted_sampling:
    weights:
      HyphenGenerator: 1
      AbbreviationGenerator: 1
      FlagAffixGenerator: 1
      PermuteGenerator: 1
      PrefixGenerator: 1
      SuffixGenerator: 1
      WordnetSynonymsGenerator: 1
      W2VGenerator: 1
      CategoriesGenerator: 1
      RandomGenerator: 1
      SecondaryMatcher: 1
      Wikipedia2VGenerator: 1
      SpecialCharacterAffixGenerator: 1
      SubstringMatchGenerator: 1
random_pipeline:
  name: random
  normalizers:
    - StripEthNormalizer
  tokenizers:
    - NoneTokenizer
  generators:
    - RandomGenerator
  filters:
    - SubnameFilter
    - ValidNameFilter
  